{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# MobileViT Deepfake Detection Training\n",
    "\n",
    "This notebook trains a MobileViT model for deepfake detection using the Real and Fake Images Dataset.\n",
    "\n",
    "**Dataset**: Real and Fake Images Dataset for Image Forensics  \n",
    "**Model**: MobileViT-S optimized for deepfake detection  \n",
    "**Platform**: Google Colab with GPU acceleration  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_packages"
   },
   "outputs": [],
   "source": [
    "# Install required packages for Colab\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install tensorboard\n",
    "!pip install timm\n",
    "!pip install einops\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib seaborn\n",
    "!pip install pyyaml\n",
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import json\n",
    "from datetime import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if we're in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Not running in Colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset_setup"
   },
   "source": [
    "## 2. Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataset_config"
   },
   "outputs": [],
   "source": [
    "# Dataset configuration for Kaggle dataset in Colab\n",
    "DATASET_PATH = \"/root/.cache/kagglehub/datasets/shivamardeshna/real-and-fake-images-dataset-for-image-forensics/versions/1\"\n",
    "\n",
    "# Verify dataset path exists\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    print(f\"Dataset found at: {DATASET_PATH}\")\n",
    "    print(f\"Dataset contents: {os.listdir(DATASET_PATH)}\")\n",
    "else:\n",
    "    print(f\"Dataset not found at: {DATASET_PATH}\")\n",
    "    print(\"Please ensure the dataset is downloaded using kagglehub\")\n",
    "    \n",
    "# Configuration dictionary\n",
    "config = {\n",
    "    # Data configuration\n",
    "    'data_dir': DATASET_PATH,\n",
    "    'batch_size': 32,\n",
    "    'image_size': 224,\n",
    "    'augmentation': 'advanced',\n",
    "    \n",
    "    # Model configuration\n",
    "    'model_name': 'mobilevit_s',\n",
    "    'num_classes': 2,\n",
    "    'pretrained': True,\n",
    "    \n",
    "    # Training configuration\n",
    "    'epochs': 50,  # Reduced for Colab\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-2,\n",
    "    'mixed_precision': True,\n",
    "    'gradient_clipping': 1.0,\n",
    "    'patience': 10,\n",
    "    \n",
    "    # System configuration\n",
    "    'num_workers': 2,  # Reduced for Colab\n",
    "    'seed': 42,\n",
    "    \n",
    "    # Logging configuration\n",
    "    'experiment_name': f\"mobilevit_deepfake_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "    'log_dir': '/content/runs',\n",
    "    'checkpoint_dir': '/content/checkpoints',\n",
    "    'results_dir': '/content/results'\n",
    "}\n",
    "\n",
    "print(f\"Experiment: {config['experiment_name']}\")\n",
    "print(f\"Epochs: {config['epochs']}\")\n",
    "print(f\"Batch size: {config['batch_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utility_functions"
   },
   "source": [
    "## 3. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_functions"
   },
   "outputs": [],
   "source": [
    "def setup_directories(config: dict):\n",
    "    \"\"\"Create necessary directories for logging, checkpoints, and results.\"\"\"\n",
    "    directories = ['log_dir', 'checkpoint_dir', 'results_dir']\n",
    "    \n",
    "    for dir_key in directories:\n",
    "        if dir_key in config:\n",
    "            os.makedirs(config[dir_key], exist_ok=True)\n",
    "            print(f\"Created directory: {config[dir_key]}\")\n",
    "\n",
    "\n",
    "def set_random_seeds(seed: int):\n",
    "    \"\"\"Set random seeds for reproducible results.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    print(f\"Random seed set to: {seed}\")\n",
    "\n",
    "\n",
    "def setup_device_and_memory():\n",
    "    \"\"\"Setup device and memory optimizations.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        \n",
    "        # Clear cache\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"Using CPU\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "\n",
    "# Setup environment\n",
    "setup_directories(config)\n",
    "set_random_seeds(config['seed'])\n",
    "device = setup_device_and_memory()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_definition"
   },
   "source": [
    "## 4. Model Definition\n",
    "\n",
    "Since we can't import from local modules in Colab, we'll define a simplified MobileViT model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_code"
   },
   "outputs": [],
   "source": [
    "# Simplified MobileViT implementation for Colab\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.SiLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "class MobileViTBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, patch_size=2, num_heads=4, mlp_ratio=2):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Local representation\n",
    "        self.local_conv = nn.Sequential(\n",
    "            ConvBlock(in_channels, in_channels),\n",
    "            ConvBlock(in_channels, out_channels, kernel_size=1, padding=0)\n",
    "        )\n",
    "        \n",
    "        # Global representation with self-attention\n",
    "        self.transformer = nn.TransformerEncoderLayer(\n",
    "            d_model=out_channels,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=out_channels * mlp_ratio,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Fusion\n",
    "        self.fusion = ConvBlock(out_channels, out_channels, kernel_size=1, padding=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Local representation\n",
    "        local_rep = self.local_conv(x)\n",
    "        \n",
    "        # Patch embedding for transformer\n",
    "        B, C, H, W = local_rep.shape\n",
    "        patch_h, patch_w = H // self.patch_size, W // self.patch_size\n",
    "        \n",
    "        # Reshape for transformer\n",
    "        patches = rearrange(local_rep, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', \n",
    "                          p1=self.patch_size, p2=self.patch_size)\n",
    "        \n",
    "        # Apply transformer\n",
    "        global_rep = self.transformer(patches)\n",
    "        \n",
    "        # Reshape back\n",
    "        global_rep = rearrange(global_rep, 'b (h w) (p1 p2 c) -> b c (h p1) (w p2)',\n",
    "                             h=patch_h, w=patch_w, p1=self.patch_size, p2=self.patch_size)\n",
    "        \n",
    "        # Fusion\n",
    "        output = self.fusion(global_rep + local_rep)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class SimplifiedMobileViT(nn.Module):\n",
    "    def __init__(self, num_classes=2, image_size=224):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Stem\n",
    "        self.stem = nn.Sequential(\n",
    "            ConvBlock(3, 32, stride=2),\n",
    "            ConvBlock(32, 64)\n",
    "        )\n",
    "        \n",
    "        # MobileViT stages\n",
    "        self.stage1 = nn.Sequential(\n",
    "            ConvBlock(64, 96),\n",
    "            MobileViTBlock(96, 128)\n",
    "        )\n",
    "        \n",
    "        self.stage2 = nn.Sequential(\n",
    "            ConvBlock(128, 144, stride=2),\n",
    "            MobileViTBlock(144, 192)\n",
    "        )\n",
    "        \n",
    "        self.stage3 = nn.Sequential(\n",
    "            ConvBlock(192, 240, stride=2),\n",
    "            MobileViTBlock(240, 320)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(320, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        \n",
    "        x = self.global_pool(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "def create_model(config: dict) -> nn.Module:\n",
    "    \"\"\"Create and initialize the model.\"\"\"\n",
    "    num_classes = config.get('num_classes', 2)\n",
    "    image_size = config.get('image_size', 224)\n",
    "    \n",
    "    print(f\"Creating MobileViT model with {num_classes} classes...\")\n",
    "    model = SimplifiedMobileViT(num_classes=num_classes, image_size=image_size)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = create_model(config)\n",
    "model = model.to(device)\n",
    "print(\"Model created and moved to device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_loading"
   },
   "source": [
    "## 5. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_code"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "\n",
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, split='train'):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        \n",
    "        # Find image files\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Look for real and fake subdirectories\n",
    "        real_dir = os.path.join(data_dir, 'real')\n",
    "        fake_dir = os.path.join(data_dir, 'fake')\n",
    "        \n",
    "        # If direct real/fake dirs don't exist, look for other patterns\n",
    "        if not os.path.exists(real_dir) or not os.path.exists(fake_dir):\n",
    "            print(f\"Looking for image files in: {data_dir}\")\n",
    "            all_dirs = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "            print(f\"Found directories: {all_dirs}\")\n",
    "            \n",
    "            # Try to find real and fake patterns\n",
    "            for dir_name in all_dirs:\n",
    "                dir_path = os.path.join(data_dir, dir_name)\n",
    "                if 'real' in dir_name.lower() or 'authentic' in dir_name.lower():\n",
    "                    real_dir = dir_path\n",
    "                elif 'fake' in dir_name.lower() or 'manipulated' in dir_name.lower():\n",
    "                    fake_dir = dir_path\n",
    "        \n",
    "        # Load real images\n",
    "        if os.path.exists(real_dir):\n",
    "            real_images = glob.glob(os.path.join(real_dir, '**/*.jpg'), recursive=True) + \\\n",
    "                         glob.glob(os.path.join(real_dir, '**/*.png'), recursive=True) + \\\n",
    "                         glob.glob(os.path.join(real_dir, '**/*.jpeg'), recursive=True)\n",
    "            self.images.extend(real_images)\n",
    "            self.labels.extend([0] * len(real_images))  # 0 for real\n",
    "            print(f\"Found {len(real_images)} real images\")\n",
    "        \n",
    "        # Load fake images\n",
    "        if os.path.exists(fake_dir):\n",
    "            fake_images = glob.glob(os.path.join(fake_dir, '**/*.jpg'), recursive=True) + \\\n",
    "                         glob.glob(os.path.join(fake_dir, '**/*.png'), recursive=True) + \\\n",
    "                         glob.glob(os.path.join(fake_dir, '**/*.jpeg'), recursive=True)\n",
    "            self.images.extend(fake_images)\n",
    "            self.labels.extend([1] * len(fake_images))  # 1 for fake\n",
    "            print(f\"Found {len(fake_images)} fake images\")\n",
    "        \n",
    "        print(f\"Total {split} images: {len(self.images)}\")\n",
    "        \n",
    "        if len(self.images) == 0:\n",
    "            raise ValueError(f\"No images found in {data_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load image\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a blank image if loading fails\n",
    "            image = Image.new('RGB', (224, 224), color='black')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "def get_transforms(image_size=224, augmentation='advanced'):\n",
    "    \"\"\"Get data transforms for training and validation.\"\"\"\n",
    "    \n",
    "    # Base transforms\n",
    "    base_transforms = [\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    "    \n",
    "    # Training transforms with augmentation\n",
    "    if augmentation == 'advanced':\n",
    "        train_transforms = transforms.Compose([\n",
    "            transforms.Resize((int(image_size * 1.1), int(image_size * 1.1))),\n",
    "            transforms.RandomCrop((image_size, image_size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        train_transforms = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    # Validation transforms (no augmentation)\n",
    "    val_transforms = transforms.Compose(base_transforms)\n",
    "    \n",
    "    return train_transforms, val_transforms\n",
    "\n",
    "\n",
    "def create_data_loaders(config):\n",
    "    \"\"\"Create data loaders for training and validation.\"\"\"\n",
    "    \n",
    "    train_transforms, val_transforms = get_transforms(\n",
    "        image_size=config['image_size'],\n",
    "        augmentation=config['augmentation']\n",
    "    )\n",
    "    \n",
    "    # For this demo, we'll use the same directory and split manually\n",
    "    # In practice, you'd have separate train/val/test directories\n",
    "    dataset = DeepfakeDataset(\n",
    "        data_dir=config['data_dir'],\n",
    "        transform=train_transforms,\n",
    "        split='full'\n",
    "    )\n",
    "    \n",
    "    # Split dataset into train/val (80/20)\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(0.8 * total_size)\n",
    "    val_size = total_size - train_size\n",
    "    \n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size]\n",
    "    )\n",
    "    \n",
    "    # Update validation dataset transform\n",
    "    val_dataset.dataset.transform = val_transforms\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "try:\n",
    "    train_loader, val_loader = create_data_loaders(config)\n",
    "    print(\"Data loaders created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating data loaders: {e}\")\n",
    "    print(\"Please check the dataset path and structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_setup"
   },
   "source": [
    "## 6. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_code"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "class DeepfakeTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, config):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        \n",
    "        # Setup optimizer and scheduler\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay']\n",
    "        )\n",
    "        \n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer,\n",
    "            T_max=config['epochs'],\n",
    "            eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        # Loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Mixed precision scaler\n",
    "        self.scaler = torch.cuda.amp.GradScaler() if config['mixed_precision'] else None\n",
    "        \n",
    "        # Training state\n",
    "        self.current_epoch = 0\n",
    "        self.best_f1 = 0.0\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_f1_scores = []\n",
    "        self.patience_counter = 0\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = len(self.train_loader)\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            if self.config['mixed_precision']:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    output = self.model(data)\n",
    "                    loss = self.criterion(output, target)\n",
    "                \n",
    "                self.scaler.scale(loss).backward()\n",
    "                \n",
    "                if self.config['gradient_clipping'] > 0:\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config['gradient_clipping'])\n",
    "                \n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "            else:\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "                loss.backward()\n",
    "                \n",
    "                if self.config['gradient_clipping'] > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config['gradient_clipping'])\n",
    "                \n",
    "                self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Print progress\n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f'Epoch {self.current_epoch}, Batch {batch_idx}/{num_batches}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        self.train_losses.append(avg_loss)\n",
    "        return avg_loss\n",
    "    \n",
    "    def validate(self):\n",
    "        \"\"\"Validate the model.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in self.val_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                \n",
    "                if self.config['mixed_precision']:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        output = self.model(data)\n",
    "                        loss = self.criterion(output, target)\n",
    "                else:\n",
    "                    output = self.model(data)\n",
    "                    loss = self.criterion(output, target)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Get predictions\n",
    "                pred = output.argmax(dim=1)\n",
    "                all_predictions.extend(pred.cpu().numpy())\n",
    "                all_targets.extend(target.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        accuracy = accuracy_score(all_targets, all_predictions)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_targets, all_predictions, average='weighted')\n",
    "        \n",
    "        self.val_losses.append(avg_loss)\n",
    "        self.val_f1_scores.append(f1)\n",
    "        \n",
    "        return {\n",
    "            'loss': avg_loss,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "    \n",
    "    def save_checkpoint(self, filepath, is_best=False):\n",
    "        \"\"\"Save model checkpoint.\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': self.current_epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'best_f1': self.best_f1,\n",
    "            'config': self.config\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, filepath)\n",
    "        \n",
    "        if is_best:\n",
    "            best_path = os.path.join(os.path.dirname(filepath), 'best_model.pth')\n",
    "            torch.save(checkpoint, best_path)\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Main training loop.\"\"\"\n",
    "        print(f\"Starting training for {self.config['epochs']} epochs...\")\n",
    "        \n",
    "        for epoch in range(self.config['epochs']):\n",
    "            self.current_epoch = epoch\n",
    "            \n",
    "            # Train\n",
    "            train_loss = self.train_epoch()\n",
    "            \n",
    "            # Validate\n",
    "            val_metrics = self.validate()\n",
    "            \n",
    "            # Update scheduler\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Print epoch results\n",
    "            print(f\"\\nEpoch {epoch}/{self.config['epochs']}:\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Loss: {val_metrics['loss']:.4f}\")\n",
    "            print(f\"Val Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "            print(f\"Val F1: {val_metrics['f1']:.4f}\")\n",
    "            print(f\"Learning Rate: {self.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            is_best = val_metrics['f1'] > self.best_f1\n",
    "            if is_best:\n",
    "                self.best_f1 = val_metrics['f1']\n",
    "                self.patience_counter = 0\n",
    "                print(f\"New best F1 score: {self.best_f1:.4f}\")\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "            \n",
    "            # Save checkpoint\n",
    "            checkpoint_path = os.path.join(self.config['checkpoint_dir'], f'checkpoint_epoch_{epoch}.pth')\n",
    "            self.save_checkpoint(checkpoint_path, is_best)\n",
    "            \n",
    "            # Early stopping\n",
    "            if self.patience_counter >= self.config['patience']:\n",
    "                print(f\"Early stopping triggered after {epoch} epochs\")\n",
    "                break\n",
    "            \n",
    "            print(\"-\" * 60)\n",
    "        \n",
    "        return {\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses,\n",
    "            'val_f1_scores': self.val_f1_scores\n",
    "        }\n",
    "\n",
    "\n",
    "# Create trainer\n",
    "trainer = DeepfakeTrainer(model, train_loader, val_loader, config)\n",
    "print(\"Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_execution"
   },
   "source": [
    "## 7. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start_training"
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Mixed precision: {config['mixed_precision']}\")\n",
    "print(f\"Batch size: {config['batch_size']}\")\n",
    "print(f\"Learning rate: {config['learning_rate']}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train the model\n",
    "training_history = trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Best validation F1 score: {trainer.best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualization"
   },
   "source": [
    "## 8. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_results"
   },
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot losses\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(training_history['train_losses'], label='Train Loss')\n",
    "plt.plot(training_history['val_losses'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot F1 scores\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(training_history['val_f1_scores'], label='Validation F1', color='orange')\n",
    "plt.title('Validation F1 Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot learning rate (if available)\n",
    "plt.subplot(1, 3, 3)\n",
    "epochs = range(len(training_history['train_losses']))\n",
    "lrs = [trainer.scheduler.get_last_lr()[0] for _ in epochs]\n",
    "plt.plot(epochs, lrs, label='Learning Rate', color='green')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config['results_dir'], 'training_history.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training plots saved to {config['results_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## 9. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_model"
   },
   "outputs": [],
   "source": [
    "# Load best model for evaluation\n",
    "best_model_path = os.path.join(config['checkpoint_dir'], 'best_model.pth')\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    print(f\"Loading best model from {best_model_path}\")\n",
    "    checkpoint = torch.load(best_model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Best model F1 score: {checkpoint['best_f1']:.4f}\")\n",
    "else:\n",
    "    print(\"No best model checkpoint found, using current model\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        with torch.cuda.amp.autocast() if config['mixed_precision'] else torch.no_grad():\n",
    "            output = model(data)\n",
    "        \n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        pred = output.argmax(dim=1)\n",
    "        \n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "        all_targets.extend(target.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "# Calculate final metrics\n",
    "final_accuracy = accuracy_score(all_targets, all_predictions)\n",
    "final_precision, final_recall, final_f1, _ = precision_recall_fscore_support(\n",
    "    all_targets, all_predictions, average='weighted'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {final_accuracy:.4f}\")\n",
    "print(f\"Precision: {final_precision:.4f}\")\n",
    "print(f\"Recall: {final_recall:.4f}\")\n",
    "print(f\"F1 Score: {final_f1:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(all_targets, all_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.savefig(os.path.join(config['results_dir'], 'confusion_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save final results\n",
    "final_results = {\n",
    "    'experiment_name': config['experiment_name'],\n",
    "    'final_metrics': {\n",
    "        'accuracy': float(final_accuracy),\n",
    "        'precision': float(final_precision),\n",
    "        'recall': float(final_recall),\n",
    "        'f1': float(final_f1)\n",
    "    },\n",
    "    'training_config': config,\n",
    "    'best_epoch_f1': float(trainer.best_f1)\n",
    "}\n",
    "\n",
    "results_path = os.path.join(config['results_dir'], 'final_results.json')\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to: {results_path}\")\n",
    "print(f\"All outputs saved to: {config['results_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_results"
   },
   "source": [
    "## 10. Download Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_files"
   },
   "outputs": [],
   "source": [
    "# Compress results for download\n",
    "import zipfile\n",
    "\n",
    "def create_results_zip():\n",
    "    zip_path = '/content/deepfake_detection_results.zip'\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "        # Add results directory\n",
    "        for root, dirs, files in os.walk(config['results_dir']):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arc_name = os.path.relpath(file_path, '/content')\n",
    "                zipf.write(file_path, arc_name)\n",
    "        \n",
    "        # Add best model checkpoint\n",
    "        best_model_path = os.path.join(config['checkpoint_dir'], 'best_model.pth')\n",
    "        if os.path.exists(best_model_path):\n",
    "            arc_name = os.path.relpath(best_model_path, '/content')\n",
    "            zipf.write(best_model_path, arc_name)\n",
    "    \n",
    "    print(f\"Results compressed to: {zip_path}\")\n",
    "    return zip_path\n",
    "\n",
    "# Create zip file\n",
    "zip_path = create_results_zip()\n",
    "\n",
    "# Download in Colab\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    print(\"Downloading results...\")\n",
    "    files.download(zip_path)\n",
    "else:\n",
    "    print(f\"Results available at: {zip_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"Final F1 Score: {final_f1:.4f}\")\n",
    "print(f\"Experiment: {config['experiment_name']}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}